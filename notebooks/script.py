"""
ğŸ  SalesHouses - PrÃ©diction de Prix Immobiliers au Maroc
Pipeline ML Complet Sans Data Leakage

Structure:
1. Importations et Configuration
2. Chargement des DonnÃ©es
3. Analyse Exploratoire (EDA)
4. PrÃ©traitement et Feature Engineering
5. Suppression des Outliers
6. PrÃ©paration pour la ModÃ©lisation
7. EntraÃ®nement et Ã‰valuation des ModÃ¨les
8. Validation CroisÃ©e
9. Optimisation des HyperparamÃ¨tres
10. Sauvegarde du ModÃ¨le
11. Fonction de PrÃ©diction

Author: SalesHouses Team
Date: Janvier 2026
"""

# ============================================================================
# 1. IMPORTATIONS ET CONFIGURATION
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import joblib
import json
from datetime import datetime

# Configuration des graphiques
plt.style.use('seaborn-v0_8-darkgrid')
plt.rcParams['figure.figsize'] = (15, 6)

print("="*80)
print("ğŸ  SALESHOUSES - PRÃ‰DICTION DE PRIX IMMOBILIERS AU MAROC")
print("="*80)
print(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# Create necessary directories
import os
os.makedirs('../data/processed', exist_ok=True)
os.makedirs('../visualizations', exist_ok=True)
os.makedirs('../models', exist_ok=True)
os.makedirs('../reports', exist_ok=True)

# ============================================================================
# 2. CHARGEMENT DES DONNÃ‰ES
# ============================================================================

print("="*80)
print("ğŸ“‚ Ã‰TAPE 1: CHARGEMENT DES DONNÃ‰ES")
print("="*80)

# Charger le dataset
df1 = pd.read_csv('../data/appartements-data-db.csv')
print(f"âœ… Dataset chargÃ© : {df1.shape}")
print(f"\nğŸ“Š AperÃ§u des premiÃ¨res lignes:")
print(df1.head())

print(f"\nğŸ“‹ Informations sur les colonnes:")
print(df1.info())

print(f"\nğŸ“ Dimensions du dataset:")
print(f"  Lignes : {df1.shape[0]}")
print(f"  Colonnes : {df1.shape[1]}")

# ============================================================================
# 3. ANALYSE EXPLORATOIRE (EDA)
# ============================================================================

print("\n" + "="*80)
print("ğŸ“Š Ã‰TAPE 2: ANALYSE EXPLORATOIRE DES DONNÃ‰ES (EDA)")
print("="*80)

# Supprimer les colonnes inutiles pour l'analyse
df2 = df1.drop(['title', 'link'], axis='columns', errors='ignore')
print(f"âœ… Colonnes inutiles supprimÃ©es")
print(f"ğŸ“Š Nouvelles dimensions : {df2.shape}")

# Convert price to numeric
if 'price' in df2.columns:
    df2['price'] = df2['price'].astype(str).str.replace(r'[^\d.]', '', regex=True)
    df2['price'] = pd.to_numeric(df2['price'], errors='coerce')
    print("âœ… Price column converted to numeric")

# VÃ©rifier les valeurs manquantes
print(f"\nğŸ” Valeurs manquantes par colonne:")
missing_values = df2.isnull().sum()
for col, count in missing_values[missing_values > 0].items():
    print(f"  {col:20s}: {count:5d} ({count/len(df2)*100:5.2f}%)")

# Supprimer les doublons
duplicates = df2.duplicated().sum()
if duplicates > 0:
    df2 = df2.drop_duplicates()
    print(f"\nğŸ—‘ï¸  {duplicates} doublons supprimÃ©s")
    print(f"ğŸ“Š Forme aprÃ¨s suppression : {df2.shape}")

# Statistiques descriptives
print(f"\nğŸ“ˆ Statistiques descriptives des variables numÃ©riques:")
print(df2.describe())

# Visualisation 1: Distribution des prix
print(f"\nğŸ“Š GÃ©nÃ©ration des visualisations...")

fig, axes = plt.subplots(1, 2, figsize=(15, 5))


axes[0].hist(df2['price'].dropna(), bins=50, color='steelblue', edgecolor='black', alpha=0.7)
axes[0].set_xlabel('Prix (DH)', fontsize=12)
axes[0].set_ylabel('FrÃ©quence', fontsize=12)
axes[0].set_title('Distribution des Prix', fontsize=14, fontweight='bold')
axes[0].grid(True, alpha=0.3)

axes[1].boxplot(df2['price'].dropna(), vert=False)
axes[1].set_xlabel('Prix (DH)', fontsize=12)
axes[1].set_title('Box Plot des Prix', fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('../visualizations/model_comparison.png', dpi=300, bbox_inches='tight')
plt.close()
print("  âœ… ../visualizations/model_comparison.png")


# ============================================================================
# 4. PRÃ‰TRAITEMENT ET FEATURE ENGINEERING
# ============================================================================

print("\n" + "="*80)
print("ğŸ”§ Ã‰TAPE 3: PRÃ‰TRAITEMENT ET FEATURE ENGINEERING")
print("="*80)

# Copier le dataframe
df3 = df2.copy()

# 4.1 Traduction des noms de villes (arabe â†’ franÃ§ais)
city_map = {
    "Ø§Ù„Ø¯Ø§Ø± Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡": "Casablanca", "Ø¯Ø§Ø± Ø¨ÙˆØ¹Ø²Ø©": "Dar Bouazza",
    "Ø§Ù„Ø±Ø¨Ø§Ø·": "Rabat", "Ù…Ø±Ø§ÙƒØ´": "Marrakech", "Ø£ØµÙŠÙ„Ø©": "Asilah",
    "Ø¨ÙˆØ³ÙƒÙˆØ±Ø©": "Bouskoura", "Ø§Ù„Ù‚Ù†ÙŠØ·Ø±Ø©": "KÃ©nitra", "Ø§Ù„Ù…Ø­Ù…Ø¯ÙŠØ©": "Mohammedia",
    "Ø£ÙƒØ§Ø¯ÙŠØ±": "Agadir", "ØªÙ…Ø§Ø±Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©": "Tamesna", "Ø³Ù„Ø§": "SalÃ©",
    "Ø­Ø¯ Ø³ÙˆØ§Ù„Ù…": "Had Soualem", "ØªÙ…Ø§Ø±Ø©": "Temara", "Ø¨Ù† Ø³Ù„ÙŠÙ…Ø§Ù†": "Benslimane",
    "Ø·Ù†Ø¬Ø©": "Tanger", "Ø¨ÙˆØ²Ù†ÙŠÙ‚Ø©": "Bouznika", "Ù…ÙƒÙ†Ø§Ø³": "MeknÃ¨s",
    "ÙØ§Ø³": "FÃ¨s", "Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©": "El Jadida", "Ø§Ù„Ù…Ù†ØµÙˆØ±ÙŠØ©": "El Mansouria",
    "Ù…Ø±ØªÙŠÙ„": "Martil", "Ø§Ù„ÙÙ†ÙŠØ¯Ù‚": "Fnideq", "ØªØ·ÙˆØ§Ù†": "TÃ©touan",
    "Ø§Ù„Ø³Ø¹ÙŠØ¯ÙŠØ©": "Saidia", "Ø§Ù„Ù†ÙˆØ§ØµØ±": "Nouaceur", "ØªÙ…Ø§Ø±ÙŠØ³": "Tamaris",
    "ÙƒØ§Ø¨Ùˆ Ù†ÙŠÙƒØ±Ùˆ": "Cabo Negro", "Ø³ÙŠØ¯ÙŠ Ø¹Ù„Ø§Ù„ Ø§Ù„Ø¨Ø­Ø±Ø§ÙˆÙŠ": "Sidi Allal El Bahraoui",
    "Ø¨Ù†ÙŠ Ù…Ù„Ø§Ù„": "BÃ©ni Mellal", "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ": "Unknown", "Ø§Ù„ØµÙˆÙŠØ±Ø©": "Essaouira",
    "Ø§Ù„Ù…Ù‡Ø¯ÙŠØ©": "Mehdia", "ÙˆØ¬Ø¯Ø©": "Oujda", "ÙˆØ§Ø¯ÙŠ Ù„Ø§Ùˆ": "Oued Laou",
    "Ø§Ù„Ø¯Ø´ÙŠØ±Ø©": "Dcheira", "Ø³ÙŠØ¯ÙŠ Ø±Ø­Ø§Ù„": "Sidi Rahal", "Ø¯Ø±ÙˆØ©": "Deroua",
    "Ø¹ÙŠÙ† Ø¹ØªÙŠÙ‚": "Ain Attig", "Ø¢Ø³ÙÙŠ": "Safi", "Ø¥Ù†Ø²ÙƒØ§Ù†": "Inzegan",
    "Ø¥ÙØ±Ø§Ù†": "Ifrane", "Ø§Ù„Ø¯Ø§Ø®Ù„Ø©": "Dakhla", "Ø§Ù„Ø¯Ø´ÙŠØ±Ø© Ø§Ù„Ø¬Ù‡Ø§Ø¯ÙŠØ©": "DcheÃ¯ra El Jihadia",
    "ØªØºØ§Ø²ÙˆØª": "Taghazout", "Ø³ÙŠØ¯ÙŠ Ø¨ÙˆÙƒÙ†Ø§Ø¯Ù„": "Sidi Bouknadel", "Ø§Ù„ØµØ®ÙŠØ±Ø§Øª": "Skhirat",
    "Ø®Ø±ÙŠØ¨ÙƒØ©": "Khouribga", "Ø¨Ø±ÙƒØ§Ù†": "Berkane", "Ù…Ø±Ø³ Ø§Ù„Ø®ÙŠØ±": "Mers El Kheir",
    "Ø¨Ø±Ø´ÙŠØ¯": "Berrechid", "ØªÙŠØ²Ù†ÙŠØª": "Tiznit", "Ø£ÙƒØ§Ø¯ÙŠØ± Ù…Ù„ÙˆÙ„": "Agadir Melloul",
    "Ø§Ù„Ù†Ø§Ø¸ÙˆØ±": "Nador", "Ø§Ù„Ù…Ù†Ø²Ù‡": "El Menzeh", "Ø¨Ù†ÙŠ Ø£Ù†ØµØ§Ø±": "Bni Ansar",
    "Ø§Ù„Ù…Ø¶ÙŠÙ‚": "Mdiq", "ØªÙŠØ· Ù…Ù„ÙŠÙ„": "Tit Mellil", "Ø³ÙˆÙ‚ Ø£Ø±Ø¨Ø¹Ø§Ø¡": "Souk El Arbaa",
    "Ø¨ÙŠÙˆÚ­Ø±Ù‰": "Biougra", "Ø³Ø·Ø§Øª": "Settat", "Ø¹ÙŠÙ† Ø¹ÙˆØ¯Ø©": "Ain Aouda",
    "ØªØ§Ø²Ø©": "Taza", "Ø§Ù„Ø®Ù…ÙŠØ³Ø§Øª": "Khemisset", "ÙˆØ§Ø¯ÙŠ Ø²Ù…": "Oued Zem",
    "ØµÙØ±Ùˆ": "Sefrou", "Ù…Ø±Ø²ÙˆÙƒØ©": "Merzouga", "Ø§Ù„Ø­Ø§Ø¬Ø¨": "El Hajeb",
    "Ø³Ù„ÙˆØ§Ù†": "Selouane", "ØªØ§ÙˆÙ†Ø§Øª": "Taounate", "Ø³ÙŠØ¯ÙŠ Ø¨Ù†ÙˆØ±": "Sidi Bennour",
    "Ø§Ù„Ù‚ØµÙŠØ¨Ø©": "El Ksiba"
}

df3['city_name'] = df3['city_name'].replace(city_map)
df3['city_name'] = df3['city_name'].str.strip()
print("âœ… Noms de villes traduits et standardisÃ©s")

# 4.2 Extraction des Ã©quipements (one-hot encoding)
if 'equipment' in df3.columns:
    equipment_dummies = df3['equipment'].str.get_dummies(sep='/')
    df3 = pd.concat([df3, equipment_dummies], axis=1)
    df3 = df3.drop('equipment', axis=1)
    equipment_features = list(equipment_dummies.columns)
    print(f"âœ… {len(equipment_features)} Ã©quipements extraits : {equipment_features}")

# 4.3 Suppression des lignes sans prix (variable cible)
initial_len = len(df3)
df3 = df3.dropna(subset=['price'])
removed = initial_len - len(df3)
if removed > 0:
    print(f"âœ… {removed} lignes sans prix supprimÃ©es")

# 4.4 Imputation des valeurs manquantes
# Variables numÃ©riques : remplacer par la mÃ©diane
num_cols = ['salon', 'nb_rooms', 'nb_baths', 'surface_area']
for col in num_cols:
    if col in df3.columns:
        median_val = df3[col].median()
        df3[col] = df3[col].fillna(median_val)
        print(f"âœ… {col} : valeurs manquantes remplacÃ©es par la mÃ©diane ({median_val})")

# 4.5 Feature Engineering (SANS LEAKAGE!)
print(f"\nğŸ§  CrÃ©ation de nouvelles features (sans data leakage):")

# Total rooms = nb_rooms + salon
if 'nb_rooms' in df3.columns and 'salon' in df3.columns:
    df3['total_rooms'] = df3['nb_rooms'] + df3['salon']
    print("  âœ… total_rooms = nb_rooms + salon")

# Bath to room ratio (proportion of bathrooms relative to total rooms)
# This helps capture the luxury/comfort level of the apartment
if 'nb_baths' in df3.columns and 'total_rooms' in df3.columns:
    df3['bath_room_ratio'] = df3['nb_baths'] / (df3['total_rooms'] + 1)
    print("  âœ… bath_room_ratio = nb_baths / (total_rooms + 1)")

# Surface per room
if 'surface_area' in df3.columns and 'total_rooms' in df3.columns:
    df3['surface_per_room'] = df3['surface_area'] / (df3['total_rooms'] + 1)
    print("  âœ… surface_per_room = surface_area / (total_rooms + 1)")

# Equipment score
if equipment_features:
    df3['equipment_score'] = df3[equipment_features].sum(axis=1)
    print("  âœ… equipment_score = somme des Ã©quipements")

# ğŸ”´ PAS DE price_per_m2 ICI! (Ã©viter le leakage)
print("\nâš ï¸  NOTE IMPORTANTE: price_per_m2 N'EST PAS CRÃ‰Ã‰ (Ã©viter data leakage)")

print(f"\nğŸ“Š Forme aprÃ¨s feature engineering : {df3.shape}")

# ============================================================================
# 5. SUPPRESSION DES OUTLIERS
# ============================================================================

print("\n" + "="*80)
print("ğŸ¯ Ã‰TAPE 4: SUPPRESSION DES OUTLIERS")
print("="*80)

df4 = df3.copy()
initial_len = len(df4)

# 5.1 RÃ¨gles mÃ©tier
print("\nğŸ“ Application des rÃ¨gles mÃ©tier:")

# Surface entre 20 et 300 mÂ²
if 'surface_area' in df4.columns:
    df4 = df4[(df4['surface_area'] >= 20) & (df4['surface_area'] <= 300)]
    print("  âœ… Surface: [20, 300] mÂ²")

# Nombre de chambres entre 1 et 8
if 'nb_rooms' in df4.columns:
    df4 = df4[(df4['nb_rooms'] >= 1) & (df4['nb_rooms'] <= 8)]
    print("  âœ… Chambres: [1, 8]")

# Nombre de salles de bain entre 0 et 6
if 'nb_baths' in df4.columns:
    df4 = df4[(df4['nb_baths'] >= 0) & (df4['nb_baths'] <= 6)]
    print("  âœ… Salles de bain: [0, 6]")

# Minimum 10 mÂ² par piÃ¨ce
if 'surface_area' in df4.columns and 'total_rooms' in df4.columns:
    df4 = df4[df4['surface_area'] / df4['total_rooms'] >= 10]
    print("  âœ… Minimum 10 mÂ²/piÃ¨ce")

# Maximum bath = total_rooms + 2
if 'nb_baths' in df4.columns and 'total_rooms' in df4.columns:
    df4 = df4[df4['nb_baths'] <= df4['total_rooms'] + 2]
    print("  âœ… Salles de bain â‰¤ total_rooms + 2")

# 5.2 Suppression des outliers de prix (percentiles 1-99)
if 'price' in df4.columns:
    q1 = df4['price'].quantile(0.01)
    q99 = df4['price'].quantile(0.99)
    df4 = df4[(df4['price'] >= q1) & (df4['price'] <= q99)]
    print(f"\nğŸ’° Prix: [{q1:,.0f}, {q99:,.0f}] DH (percentiles 1-99)")

# 5.3 Suppression des outliers de prix PAR VILLE (IQR)
# ğŸ”´ ATTENTION: Utiliser price directement, PAS price_per_m2!
print(f"\nğŸ™ï¸  Suppression des outliers de prix par ville (mÃ©thode IQR):")

df_out = pd.DataFrame()
for city, subdf in df4.groupby('city_name'):
    if len(subdf) < 5:
        df_out = pd.concat([df_out, subdf], ignore_index=True)
        continue
    
    # Calculer les quartiles sur le PRIX directement
    Q1 = subdf['price'].quantile(0.25)
    Q3 = subdf['price'].quantile(0.75)
    IQR = Q3 - Q1
    lower = max(Q1 - 1.5 * IQR, 0)
    upper = Q3 + 1.5 * IQR
    
    # Filtrer
    reduced_df = subdf[(subdf['price'] > lower) & (subdf['price'] <= upper)]
    df_out = pd.concat([df_out, reduced_df], ignore_index=True)

df4 = df_out
print(f"  âœ… Outliers supprimÃ©s pour {df4['city_name'].nunique()} villes")

removed = initial_len - len(df4)
print(f"\nğŸ“Š RÃ©sultat:")
print(f"  Lignes supprimÃ©es : {removed} ({removed/initial_len*100:.1f}%)")
print(f"  Forme finale : {df4.shape}")

# 5.4 Price per mÂ² outlier removal (added from reference script)
print(f"\nğŸ™ï¸  Suppression des outliers de prix par mÂ² par ville (mÃ©thode std):")

df4['price_per_m2'] = df4['price'] / df4['surface_area']

def remove_price_m2_outliers(df):
    df_out = pd.DataFrame()
    for city, subdf in df.groupby('city_name'):
        m = subdf['price_per_m2'].mean()
        st = subdf['price_per_m2'].std()
        reduced_df = subdf[(subdf['price_per_m2'] > (m - st)) & (subdf['price_per_m2'] <= (m + st))]
        df_out = pd.concat([df_out, reduced_df], ignore_index=True)
    return df_out

df4 = remove_price_m2_outliers(df4)
df4 = df4.drop(['price_per_m2'], axis='columns')
print("  âœ… Price-per-mÂ² outliers removed")

# Visualization of outliers effect
print(f"\nğŸ“Š GÃ©nÃ©ration de la visualisation des outliers...")
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Before outlier removal (using df3)
axes[0].boxplot(df3['price'].dropna(), vert=False)
axes[0].set_xlabel('Prix (DH)', fontsize=12)
axes[0].set_title('Prix Avant Suppression des Outliers', fontsize=14, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# After outlier removal (using df4)
axes[1].boxplot(df4['price'].dropna(), vert=False)
axes[1].set_xlabel('Prix (DH)', fontsize=12)
axes[1].set_title('Prix AprÃ¨s Suppression des Outliers', fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('../visualizations/outliers_before_after.png', dpi=300, bbox_inches='tight')
plt.close()
print("  âœ… ../visualizations/outliers_before_after.png")
# ============================================================================
# 6. PRÃ‰PARATION POUR LA MODÃ‰LISATION
# ============================================================================

print("\n" + "="*80)
print("ğŸ› ï¸  Ã‰TAPE 5: PRÃ‰PARATION POUR LA MODÃ‰LISATION")
print("="*80)

df5 = df4.copy()

# 6.1 Regrouper les villes rares (< 10 annonces)
print("\nğŸ“‰ Regroupement des villes rares:")
city_counts = df5['city_name'].value_counts()
rare_cities = city_counts[city_counts <= 10].index
df5['city_name'] = df5['city_name'].apply(lambda x: 'other' if x in rare_cities else x)
print(f"  âœ… {len(rare_cities)} villes regroupÃ©es dans 'other'")
print(f"  âœ… {df5['city_name'].nunique()} villes uniques restantes")

# 6.2 One-Hot Encoding des villes
print(f"\nğŸ”¢ One-Hot Encoding des villes:")
dummies = pd.get_dummies(df5['city_name'], prefix='city')
city_features = list(dummies.columns)
df5 = pd.concat([df5, dummies], axis=1)
df5 = df5.drop('city_name', axis=1)
print(f"  âœ… {len(city_features)} colonnes crÃ©Ã©es pour les villes")

# 6.3 Supprimer les colonnes temporaires
cols_to_drop = ['salon', 'nb_rooms']
df5 = df5.drop([col for col in cols_to_drop if col in df5.columns], axis=1)
print(f"\nğŸ—‘ï¸  Colonnes supprimÃ©es : {cols_to_drop}")

# 6.4 Sauvegarder le dataset nettoyÃ© (avant modÃ©lisation)
df5.to_csv('../data/processed/data_cleaned.csv', index=False)
print(f"\nğŸ’¾ Dataset nettoyÃ© sauvegardÃ© : ../data/processed/data_cleaned.csv")

# 6.5 SÃ©parer X et y
X = df5.drop('price', axis=1)
y = df5['price']

print(f"\nğŸ“Š Dimensions finales:")
print(f"  Features (X) : {X.shape}")
print(f"  Target (y) : {y.shape}")
print(f"  Colonnes : {list(X.columns)}")

# ============================================================================
# 7. SPLIT TRAIN/TEST
# ============================================================================

print("\n" + "="*80)
print("âœ‚ï¸  Ã‰TAPE 6: SÃ‰PARATION TRAIN/TEST")
print("="*80)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"âœ… Split effectuÃ© (80/20):")
print(f"  Train : {X_train.shape}")
print(f"  Test  : {X_test.shape}")

# 7.1 Normalisation des features numÃ©riques
numeric_features = ['nb_baths', 'surface_area', 'total_rooms', 
                   'bath_room_ratio', 'surface_per_room', 'equipment_score']
numeric_features = [col for col in numeric_features if col in X.columns]

scaler = StandardScaler()
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[numeric_features] = scaler.fit_transform(X_train[numeric_features])
X_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])

print(f"\nâœ… Features numÃ©riques normalisÃ©es: {numeric_features}")

# ============================================================================
# 8. ENTRAÃNEMENT DES MODÃˆLES
# ============================================================================

print("\n" + "="*80)
print("ğŸ¤– Ã‰TAPE 7: ENTRAÃNEMENT ET Ã‰VALUATION DES MODÃˆLES")
print("="*80)

# DÃ©finir les modÃ¨les Ã  tester
models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(alpha=10),
    'Lasso': Lasso(alpha=1, max_iter=5000),
    'Decision Tree': DecisionTreeRegressor(
        max_depth=15, min_samples_split=5, random_state=42
    ),
    'Random Forest': RandomForestRegressor(
        n_estimators=100, max_depth=20, min_samples_split=5,
        random_state=42, n_jobs=-1
    ),
    'Gradient Boosting': GradientBoostingRegressor(
        n_estimators=100, learning_rate=0.1, max_depth=5,
        random_state=42
    )
}

print(f"âœ… {len(models)} modÃ¨les dÃ©finis")

# EntraÃ®ner et Ã©valuer chaque modÃ¨le
results = {}

for name, model in models.items():
    print(f"\nğŸ”¹ {name}")
    
    # EntraÃ®nement
    model.fit(X_train_scaled, y_train)
    
    # PrÃ©dictions
    y_pred_train = model.predict(X_train_scaled)
    y_pred_test = model.predict(X_test_scaled)
    
    # MÃ©triques
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    mae = mean_absolute_error(y_test, y_pred_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    mape = np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100
    
    results[name] = {
        'model': model,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'mae': mae,
        'rmse': rmse,
        'mape': mape
    }
    
    print(f"  Train RÂ² : {train_r2:.4f}")
    print(f"  Test RÂ²  : {test_r2:.4f}")
    print(f"  MAE      : {mae:,.0f} DH")
    print(f"  RMSE     : {rmse:,.0f} DH")
    print(f"  MAPE     : {mape:.2f}%")

# CrÃ©er un DataFrame de comparaison
results_df = pd.DataFrame({
    name: {
        'Train RÂ²': metrics['train_r2'],
        'Test RÂ²': metrics['test_r2'],
        'MAE': metrics['mae'],
        'RMSE': metrics['rmse'],
        'MAPE': metrics['mape']
    }
    for name, metrics in results.items()
}).T

print(f"\nğŸ“Š Tableau rÃ©capitulatif:")
print(results_df.round(4))

# ============================================================================
# 9. VALIDATION CROISÃ‰E
# ============================================================================

print("\n" + "="*80)
print("ğŸ”„ Ã‰TAPE 8: VALIDATION CROISÃ‰E (5-FOLD)")
print("="*80)

# Normaliser tout le dataset
X_full_scaled = X.copy()
X_full_scaled[numeric_features] = scaler.fit_transform(X[numeric_features])

cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

for name, model in models.items():
    print(f"\nğŸ”¹ {name}")
    
    r2_scores = cross_val_score(model, X_full_scaled, y, cv=cv, scoring='r2')
    mae_scores = -cross_val_score(model, X_full_scaled, y, cv=cv, 
                                  scoring='neg_mean_absolute_error')
    
    print(f"  RÂ²  : {r2_scores.mean():.4f} (Â±{r2_scores.std():.4f})")
    print(f"  MAE : {mae_scores.mean():,.0f} (Â±{mae_scores.std():,.0f}) DH")
    
    # Mettre Ã  jour les rÃ©sultats
    results[name]['cv_r2_mean'] = r2_scores.mean()
    results[name]['cv_mae_mean'] = mae_scores.mean()

# ============================================================================
# 10. SÃ‰LECTION DU MEILLEUR MODÃˆLE
# ============================================================================

print("\n" + "="*80)
print("ğŸ† Ã‰TAPE 9: SÃ‰LECTION DU MEILLEUR MODÃˆLE")
print("="*80)

# Trouver le meilleur modÃ¨le basÃ© sur RÂ² test
best_model_name = max(results, key=lambda k: results[k]['test_r2'])
best_model = results[best_model_name]['model']

print(f"\nğŸ¥‡ Meilleur modÃ¨le : {best_model_name}")
print(f"   Test RÂ²  : {results[best_model_name]['test_r2']:.4f}")
print(f"   MAE      : {results[best_model_name]['mae']:,.0f} DH")
print(f"   RMSE     : {results[best_model_name]['rmse']:,.0f} DH")
print(f"   MAPE     : {results[best_model_name]['mape']:.2f}%")

# Visualisation de comparaison
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# RÂ² comparison
r2_values = [results[name]['test_r2'] for name in results.keys()]
axes[0].barh(list(results.keys()), r2_values, color='steelblue')
axes[0].set_xlabel('RÂ² Score')
axes[0].set_title('Comparaison RÂ² (Test)', fontweight='bold')
axes[0].grid(True, alpha=0.3)

# MAE comparison
mae_values = [results[name]['mae'] for name in results.keys()]
axes[1].barh(list(results.keys()), mae_values, color='coral')
axes[1].set_xlabel('MAE (DH)')
axes[1].set_title('Comparaison MAE', fontweight='bold')
axes[1].grid(True, alpha=0.3)

# RMSE comparison
rmse_values = [results[name]['rmse'] for name in results.keys()]
axes[2].barh(list(results.keys()), rmse_values, color='lightgreen')
axes[2].set_xlabel('RMSE (DH)')
axes[2].set_title('Comparaison RMSE', fontweight='bold')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('../visualizations/price_distribution.png', dpi=300, bbox_inches='tight')
plt.close()
print("  âœ… ../visualizations/price_distribution.png")

# Visualisation 2: Statistiques par ville
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Top villes par nombre d'annonces
city_counts = df2['city_name'].value_counts().head(15)
axes[0, 0].barh(range(len(city_counts)), city_counts.values, color='lightcoral')
axes[0, 0].set_yticks(range(len(city_counts)))
axes[0, 0].set_yticklabels(city_counts.index)
axes[0, 0].set_xlabel("Nombre d'annonces", fontsize=11)
axes[0, 0].set_title('Top 15 Villes par Nombre d\'Annonces', fontsize=12, fontweight='bold')
axes[0, 0].invert_yaxis()
axes[0, 0].grid(True, alpha=0.3, axis='x')

# Prix moyen par ville
city_prices = df2.groupby('city_name')['price'].mean().sort_values(ascending=False).head(15)
axes[0, 1].barh(range(len(city_prices)), city_prices.values, color='lightgreen')
axes[0, 1].set_yticks(range(len(city_prices)))
axes[0, 1].set_yticklabels(city_prices.index)
axes[0, 1].set_xlabel('Prix Moyen (DH)', fontsize=11)
axes[0, 1].set_title('Top 15 Villes par Prix Moyen', fontsize=12, fontweight='bold')
axes[0, 1].invert_yaxis()
axes[0, 1].grid(True, alpha=0.3, axis='x')

# Distribution des surfaces
axes[1, 0].hist(df2['surface_area'].dropna(), bins=50, color='skyblue', edgecolor='black', alpha=0.7)
axes[1, 0].set_xlabel('Surface (mÂ²)', fontsize=11)
axes[1, 0].set_ylabel('FrÃ©quence', fontsize=11)
axes[1, 0].set_title('Distribution des Surfaces', fontsize=12, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3)

# Prix vs Surface
axes[1, 1].scatter(df2['surface_area'], df2['price'], alpha=0.5, s=20, color='coral')
axes[1, 1].set_xlabel('Surface (mÂ²)', fontsize=11)
axes[1, 1].set_ylabel('Prix (DH)', fontsize=11)
axes[1, 1].set_title('Prix vs Surface', fontsize=12, fontweight='bold')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('../visualizations/city_statistics.png', dpi=300, bbox_inches='tight')
plt.close()
print("  âœ… ../visualizations/city_statistics.png")

# Visualisation 3: Matrice de corrÃ©lation
numeric_cols = df2.select_dtypes(include=[np.number]).columns
corr_matrix = df2[numeric_cols].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, square=True, linewidths=1)
plt.title('Matrice de CorrÃ©lation', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('../visualizations/correlation_matrix.png', dpi=150, bbox_inches='tight')
plt.close()
print("  âœ… ../visualizations/correlation_matrix.png")

# RÃ©sumÃ© des statistiques
print(f"\nğŸ“ˆ RÃ©sumÃ© Statistique:")
print(f"  Nombre total d'appartements : {len(df2)}")
print(f"  Prix moyen : {df2['price'].mean():,.0f} DH")
print(f"  Prix mÃ©dian : {df2['price'].median():,.0f} DH")
print(f"  Surface moyenne : {df2['surface_area'].mean():.2f} mÂ²")
print(f"  Nombre de villes : {df2['city_name'].nunique()}")

# ============================================================================
# 11. OPTIMISATION DES HYPERPARAMÃˆTRES
# ============================================================================

print("\n" + "="*80)
print("âš™ï¸  Ã‰TAPE 10: OPTIMISATION DES HYPERPARAMÃˆTRES")
print("="*80)

# Optimiser seulement si c'est Random Forest ou Gradient Boosting
if False:  # Temporarily disabled for faster execution
    print(f"\nğŸ”§ Optimisation de {best_model_name}...")
    
    if best_model_name == 'Random Forest':
        param_grid = {
            'n_estimators': [100, 200],
            'max_depth': [15, 20, None],
            'min_samples_split': [2, 5],
            'min_samples_leaf': [1, 2]
        }
    else:  # Gradient Boosting
        param_grid = {
            'n_estimators': [100, 200],
            'learning_rate': [0.05, 0.1],
            'max_depth': [3, 5, 7],
            'subsample': [0.8, 1.0]
        }
    
    grid_search = GridSearchCV(
        best_model, param_grid,
        cv=2, scoring='r2', n_jobs=1, verbose=0
    )
    
    print("ğŸ” Recherche en cours...")
    grid_search.fit(X_full_scaled, y)
    
    print(f"\nâœ… Meilleurs paramÃ¨tres trouvÃ©s:")
    for param, value in grid_search.best_params_.items():
        print(f"  â€¢ {param}: {value}")
    
    print(f"\nğŸ“Š Meilleur score RÂ² (CV): {grid_search.best_score_:.4f}")
    
    # Comparer avec le modÃ¨le original
    optimized_model = grid_search.best_estimator_
    optimized_model.fit(X_train_scaled, y_train)
    y_pred_optimized = optimized_model.predict(X_test_scaled)
    optimized_r2 = r2_score(y_test, y_pred_optimized)
    
    print(f"\nğŸ“ˆ Comparaison:")
    print(f"  ModÃ¨le original : RÂ² = {results[best_model_name]['test_r2']:.4f}")
    print(f"  ModÃ¨le optimisÃ© : RÂ² = {optimized_r2:.4f}")
    
    # Utiliser le modÃ¨le optimisÃ© si meilleur
    if optimized_r2 > results[best_model_name]['test_r2']:
        best_model = optimized_model
        print("\nâœ… ModÃ¨le optimisÃ© sÃ©lectionnÃ©!")
    else:
        print("\nâœ… ModÃ¨le original conservÃ©")
else:
    print(f"â„¹ï¸  Optimisation temporairement dÃ©sactivÃ©e pour {best_model_name}")

# ============================================================================
# 12. SAUVEGARDE DU MODÃˆLE ET DES ARTEFACTS
# ============================================================================

print("\n" + "="*80)
print("ğŸ’¾ Ã‰TAPE 11: SAUVEGARDE DU MODÃˆLE ET DES ARTEFACTS")
print("="*80)

import os
os.makedirs('models', exist_ok=True)
os.makedirs('reports', exist_ok=True)

# 12.1 Sauvegarder le modÃ¨le
joblib.dump(best_model, '../models/best_model.pkl')
print("âœ… ModÃ¨le sauvegardÃ© : ../models/best_model.pkl")

# 12.2 Sauvegarder le scaler
joblib.dump(scaler, '../models/scaler.pkl')
print("âœ… Scaler sauvegardÃ© : ../models/scaler.pkl")

# 12.3 Calculer les statistiques par ville (SANS LEAKAGE!)
# On utilise les donnÃ©es AVANT la sÃ©paration train/test
city_statistics = {}
for city in df4['city_name'].unique() if 'city_name' in df4.columns else []:
    # Utiliser df4 (avant one-hot encoding) pour calculer les stats
    city_data = df4[df4['city_name'] == city]
    if len(city_data) > 0:
        city_statistics[city] = {
            'median_price': float(city_data['price'].median()),
            'mean_price': float(city_data['price'].mean()),
            'median_surface': float(city_data['surface_area'].median()),
            'count': int(len(city_data))
        }

# 12.4 CrÃ©er les mÃ©tadonnÃ©es
metadata = {
    'version': '1.0',
    'created_at': datetime.now().isoformat(),
    'best_model': best_model_name,
    'model_performance': {
        'test_r2': float(results[best_model_name]['test_r2']),
        'mae': float(results[best_model_name]['mae']),
        'rmse': float(results[best_model_name]['rmse']),
        'mape': float(results[best_model_name]['mape'])
    },
    'feature_names': list(X.columns),
    'numeric_features': numeric_features,
    'equipment_features': equipment_features if 'equipment_features' in locals() else [],
    'city_features': city_features if 'city_features' in locals() else [],
    'available_cities': list(city_statistics.keys()),
    'city_statistics': city_statistics,
    'global_median_price': float(y.median()),
    'global_mean_price': float(y.mean())
}

# Sauvegarder les mÃ©tadonnÃ©es
with open('../models/metadata.json', 'w', encoding='utf-8') as f:
    json.dump(metadata, f, ensure_ascii=False, indent=2)
print("âœ… MÃ©tadonnÃ©es sauvegardÃ©es : ../models/metadata.json")

# 12.5 Sauvegarder les mÃ©triques des modÃ¨les
metrics_df = pd.DataFrame(results).T
metrics_df.to_csv('../reports/model_metrics.csv')
print("âœ… MÃ©triques sauvegardÃ©es : ../reports/model_metrics.csv")

# ============================================================================
# 13. FONCTION DE PRÃ‰DICTION
# ============================================================================

print("\n" + "="*80)
print("ğŸ¯ Ã‰TAPE 12: CRÃ‰ATION DE LA FONCTION DE PRÃ‰DICTION")
print("="*80)

def predict_apartment_price(city, surface_area, nb_baths, total_rooms, equipment_list=None):
    """
    PrÃ©dit le prix d'un appartement au Maroc
    
    ParamÃ¨tres:
    -----------
    city : str
        Nom de la ville (ex: 'Casablanca', 'Rabat', 'Marrakech')
    surface_area : float
        Surface en mÂ²
    nb_baths : int
        Nombre de salles de bain
    total_rooms : int
        Nombre total de piÃ¨ces (chambres + salons)
    equipment_list : list, optional
        Liste des Ã©quipements (ex: ['Ascenseur', 'Balcon', 'Parking'])
    
    Retourne:
    ---------
    dict : RÃ©sultats de la prÃ©diction avec prix et dÃ©tails
    """
    if equipment_list is None:
        equipment_list = []
    
    # CrÃ©er le vecteur de features
    features = {}
    
    # Features numÃ©riques de base
    features['surface_area'] = surface_area
    features['nb_baths'] = nb_baths
    features['total_rooms'] = total_rooms
    
    # Features engineered (SANS utiliser le prix!)
    features['bath_room_ratio'] = nb_baths / (total_rooms + 1)
    features['surface_per_room'] = surface_area / (total_rooms + 1)
    # Equipment score: number of equipment features present (quantifies luxury/comfort level)
    features['equipment_score'] = len(equipment_list)
    
    # One-hot encoding de la ville
    city_col = f'city_{city}'
    for city_feature in metadata['city_features']:
        features[city_feature] = 1 if city_feature == city_col else 0
    
    # One-hot encoding des Ã©quipements
    for equip_feature in metadata['equipment_features']:
        features[equip_feature] = 1 if equip_feature in equipment_list else 0
    
    # Convertir en DataFrame
    X_pred = pd.DataFrame([features])
    
    # Assurer que toutes les features sont prÃ©sentes
    for feature in metadata['feature_names']:
        if feature not in X_pred.columns:
            X_pred[feature] = 0
    
    # RÃ©organiser les colonnes dans le bon ordre
    X_pred = X_pred[metadata['feature_names']]
    
    # Normaliser les features numÃ©riques
    X_pred[numeric_features] = scaler.transform(X_pred[numeric_features])
    
    # Faire la prÃ©diction
    predicted_price = best_model.predict(X_pred)[0]
    
    # Calculer le prix au mÂ² (APRÃˆS la prÃ©diction!)
    price_per_m2 = predicted_price / surface_area
    
    # PrÃ©parer le rÃ©sultat
    result = {
        'predicted_price': float(predicted_price),
        'price_per_m2': float(price_per_m2),
        'city': city,
        'surface_area': surface_area,
        'nb_baths': nb_baths,
        'total_rooms': total_rooms,
        'equipment': equipment_list,
        'confidence_interval': {
            'lower': float(predicted_price * 0.85),
            'upper': float(predicted_price * 1.15)
        }
    }
    
    return result

print("âœ… Fonction de prÃ©diction crÃ©Ã©e : predict_apartment_price()")

# ============================================================================
# 14. TESTS DE PRÃ‰DICTION
# ============================================================================

print("\n" + "="*80)
print("ğŸ§ª Ã‰TAPE 13: TESTS DE PRÃ‰DICTION")
print("="*80)

# Cas de test
test_cases = [
    {
        'city': 'Casablanca',
        'surface_area': 100,
        'nb_baths': 2,
        'total_rooms': 3,
        'equipment_list': ['Ascenseur', 'Balcon', 'Parking']
    },
    {
        'city': 'Rabat',
        'surface_area': 120,
        'nb_baths': 2,
        'total_rooms': 4,
        'equipment_list': ['Ascenseur', 'Climatisation']
    },
    {
        'city': 'Marrakech',
        'surface_area': 80,
        'nb_baths': 1,
        'total_rooms': 2,
        'equipment_list': []
    }
]

predictions_results = []

for i, case in enumerate(test_cases, 1):
    result = predict_apartment_price(**case)
    predictions_results.append(result)
    
    print(f"\nğŸ  Test {i}:")
    print(f"   ğŸ“ Ville : {result['city']}")
    print(f"   ğŸ“ Surface : {result['surface_area']}mÂ²")
    print(f"   ğŸ›ï¸  PiÃ¨ces : {result['total_rooms']}, ğŸš¿ SDB : {result['nb_baths']}")
    print(f"   ğŸ”§ Ã‰quipements : {', '.join(result['equipment']) if result['equipment'] else 'Aucun'}")
    print(f"   ğŸ’° Prix prÃ©dit : {result['predicted_price']:,.0f} DH")
    print(f"   ğŸ“Š Prix/mÂ² : {result['price_per_m2']:,.0f} DH/mÂ²")
    print(f"   ğŸ“ˆ Intervalle : [{result['confidence_interval']['lower']:,.0f} - {result['confidence_interval']['upper']:,.0f}] DH")

# Sauvegarder les prÃ©dictions de test
pd.DataFrame(predictions_results).to_json(
    '../reports/test_predictions.json',
    orient='records',
    indent=2,
    force_ascii=False
)
print("\nâœ… PrÃ©dictions de test sauvegardÃ©es : ../reports/test_predictions.json")

# ============================================================================
# 15. RÃ‰SUMÃ‰ FINAL
# ============================================================================

print("\n" + "="*80)
print("ğŸ“‹ RÃ‰SUMÃ‰ FINAL DU PIPELINE")
print("="*80)

print(f"\nğŸ“Š DonnÃ©es:")
print(f"  â€¢ Dataset initial : {df1.shape[0]} appartements")
print(f"  â€¢ Dataset final : {len(df5)} appartements")
print(f"  â€¢ Features : {len(metadata['feature_names'])}")
print(f"  â€¢ Villes : {len(metadata['available_cities'])}")
print(f"  â€¢ Ã‰quipements : {len(metadata['equipment_features'])}")

print(f"\nğŸ¯ Performance du meilleur modÃ¨le ({best_model_name}):")
print(f"  â€¢ RÂ² (test) : {results[best_model_name]['test_r2']:.4f}")
print(f"  â€¢ MAE : {results[best_model_name]['mae']:,.0f} DH")
print(f"  â€¢ RMSE : {results[best_model_name]['rmse']:,.0f} DH")
print(f"  â€¢ MAPE : {results[best_model_name]['mape']:.2f}%")

print(f"\nğŸ’¾ Fichiers sauvegardÃ©s:")
print(f"  â€¢ ../models/best_model.pkl")
print(f"  â€¢ ../models/scaler.pkl")
print(f"  â€¢ ../models/metadata.json")
print(f"  â€¢ ../data/processed/data_cleaned.csv")
print(f"  â€¢ ../reports/model_metrics.csv")
print(f"  â€¢ ../reports/test_predictions.json")
print(f"  â€¢ ../visualizations/price_distribution.png")
print(f"  â€¢ ../visualizations/city_statistics.png")
print(f"  â€¢ ../visualizations/correlation_matrix.png")
print(f"  â€¢ ../visualizations/model_comparison.png")
print(f"  â€¢ ../visualizations/outliers_before_after.png")
print("\n" + "="*80)
print("âœ… PIPELINE TERMINÃ‰ AVEC SUCCÃˆS!")
print("="*80)
print(f"â° Fin : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

print("\nğŸ¯ Utilisation de la fonction de prÃ©diction:")
print("""
# Exemple:
result = predict_apartment_price(
    city='Casablanca',
    surface_area=100,
    nb_baths=2,
    total_rooms=3,
    equipment_list=['Ascenseur', 'Balcon']
)
print(f"Prix prÃ©dit : {result['predicted_price']:,.0f} DH")
""")

print("\nğŸ“š Pour charger le modÃ¨le plus tard:")
print("""
import joblib
import json

# Charger le modÃ¨le et le scaler
model = joblib.load('models/best_model.pkl')
scaler = joblib.load('models/scaler.pkl')

# Charger les mÃ©tadonnÃ©es
with open('models/metadata.json', 'r', encoding='utf-8') as f:
    metadata = json.load(f)

# Faire des prÃ©dictions
# ... (utiliser la fonction predict_apartment_price)
""")